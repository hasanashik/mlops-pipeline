name: Deploy MLOps Pipeline

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

env:
  AWS_REGION: ap-southeast-1
  PULUMI_STACK: rfsamrat/mlops-pipeline/sandbox

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        service: [ml-inference, data-ingestion]

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'

    - name: Install dependencies and run tests
      run: |
        cd services/${{ matrix.service }}
        pip install -r requirements.txt
        pip install pytest
        
        # Check if tests directory exists
        if [ -d "tests" ]; then
          echo "Tests directory found, running pytest..."
          python -m pytest tests/ -v --tb=short
        else
          echo "Tests directory not found, running basic health test..."
          python -c "
        import sys
        sys.path.insert(0, '.')
        try:
            from app import app
            print('âœ“ App import successful')
            client = app.test_client()
            response = client.get('/health')
            print(f'âœ“ Health check: {response.status_code}')
            assert response.status_code == 200
            print('âœ“ Basic health test passed')
        except Exception as e:
            print(f'âœ— Test failed: {e}')
            sys.exit(1)
          "
        fi

  deploy-infrastructure:
    needs: test
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    outputs:
      instance_ip: ${{ steps.get-outputs.outputs.instance_ip }}
      ml_inference_repo: ${{ steps.get-outputs.outputs.ml_inference_repo }}
      data_ingestion_repo: ${{ steps.get-outputs.outputs.data_ingestion_repo }}
      unique_suffix: ${{ steps.get-outputs.outputs.unique_suffix }}

    steps:
    - uses: actions/checkout@v4

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'

    - name: Install Pulumi
      run: |
        curl -fsSL https://get.pulumi.com | sh
        echo "$HOME/.pulumi/bin" >> $GITHUB_PATH

    - name: Complete stack cleanup and fresh start
      env:
        PULUMI_ACCESS_TOKEN: ${{ secrets.PULUMI_ACCESS_TOKEN }}
      run: |
        echo "Performing complete stack cleanup..."
        
        # Clean up any MLOps ECR repositories first
        REPOS=$(aws ecr describe-repositories --query 'repositories[?contains(repositoryName, `mlops/`)].repositoryName' --output text 2>/dev/null || echo "")
        if [ ! -z "$REPOS" ]; then
          for repo in $REPOS; do
            echo "Force deleting repository: $repo"
            aws ecr delete-repository --repository-name "$repo" --force || true
          done
        fi
        
        cd infrastructure
        pip install -r requirements.txt
        pulumi login
        
        # Check if stack exists and remove it completely
        if pulumi stack ls | grep -q "${{ env.PULUMI_STACK }}"; then
          echo "Stack exists, removing completely..."
          
          # Try to select and destroy (ignore errors)
          pulumi stack select ${{ env.PULUMI_STACK }} || true
          pulumi destroy --yes --skip-preview || true
          
          # Force remove the stack entirely
          pulumi stack rm ${{ env.PULUMI_STACK }} --yes --force || true
          echo "Stack removed"
        else
          echo "No existing stack found"
        fi
        
        # Create completely fresh stack
        echo "Creating fresh stack..."
        pulumi stack init ${{ env.PULUMI_STACK }}
        echo "Fresh stack created successfully"

    - name: Deploy infrastructure
      env:
        PULUMI_ACCESS_TOKEN: ${{ secrets.PULUMI_ACCESS_TOKEN }}
        SSH_PUBLIC_KEY: ${{ secrets.SSH_PUBLIC_KEY }}
      run: |
        cd infrastructure
        pulumi stack select ${{ env.PULUMI_STACK }}
        pulumi up --yes

    - name: Get deployment outputs
      id: get-outputs
      env:
        PULUMI_ACCESS_TOKEN: ${{ secrets.PULUMI_ACCESS_TOKEN }}
      run: |
        cd infrastructure
        echo "instance_ip=$(pulumi stack output instance_public_ip)" >> $GITHUB_OUTPUT
        echo "ml_inference_repo=$(pulumi stack output ml_inference_repo_url)" >> $GITHUB_OUTPUT
        echo "data_ingestion_repo=$(pulumi stack output data_ingestion_repo_url)" >> $GITHUB_OUTPUT
        echo "unique_suffix=$(pulumi stack output unique_suffix)" >> $GITHUB_OUTPUT

  build-and-push:
    needs: deploy-infrastructure
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    strategy:
      matrix:
        service: [ml-inference, data-ingestion]

    steps:
    - uses: actions/checkout@v4

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Login to Amazon ECR
      id: login-ecr
      uses: aws-actions/amazon-ecr-login@v2

    - name: Get Pulumi outputs
      id: pulumi-outputs
      run: |
        if [ "${{ matrix.service }}" == "ml-inference" ]; then
          echo "repository_url=${{ needs.deploy-infrastructure.outputs.ml_inference_repo }}" >> $GITHUB_OUTPUT
        else
          echo "repository_url=${{ needs.deploy-infrastructure.outputs.data_ingestion_repo }}" >> $GITHUB_OUTPUT
        fi

    - name: Build, tag, and push image to Amazon ECR
      env:
        ECR_REPOSITORY: ${{ steps.pulumi-outputs.outputs.repository_url }}
        IMAGE_TAG: ${{ github.sha }}
      run: |
        cd services/${{ matrix.service }}
        
        # Debug: List files to ensure we're in the right place
        echo "Current directory contents:"
        ls -la
        
        # Build the Docker image
        docker build -t $ECR_REPOSITORY:$IMAGE_TAG .
        docker tag $ECR_REPOSITORY:$IMAGE_TAG $ECR_REPOSITORY:latest
        
        # Push images
        docker push $ECR_REPOSITORY:$IMAGE_TAG
        docker push $ECR_REPOSITORY:latest
        
        echo "Successfully built and pushed ${{ matrix.service }}"

  deploy-services:
    needs: [deploy-infrastructure, build-and-push]
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'

    steps:
    - uses: actions/checkout@v4

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Get instance details
      id: get-instance
      run: |
        echo "instance_ip=${{ needs.deploy-infrastructure.outputs.instance_ip }}" >> $GITHUB_OUTPUT
        echo "unique_suffix=${{ needs.deploy-infrastructure.outputs.unique_suffix }}" >> $GITHUB_OUTPUT

    - name: Wait for EC2 instance to be ready
      run: |
        echo "Waiting for EC2 instance to be fully ready..."
        INSTANCE_IP="${{ steps.get-instance.outputs.instance_ip }}"
        
        # Wait for SSH to be available
        timeout=300
        while ! nc -z $INSTANCE_IP 22; do
          echo "Waiting for SSH on $INSTANCE_IP..."
          sleep 10
          timeout=$((timeout-10))
          if [ $timeout -le 0 ]; then
            echo "SSH timeout reached"
            break
          fi
        done
        
        echo "SSH is available on $INSTANCE_IP"
        sleep 60  # Longer buffer time for user_data to complete

    - name: Deploy services to EC2
      uses: appleboy/ssh-action@v1.0.3
      env:
        AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
        AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        AWS_DEFAULT_REGION: ap-southeast-1
        UNIQUE_SUFFIX: ${{ steps.get-instance.outputs.unique_suffix }}
      with:
        host: ${{ steps.get-instance.outputs.instance_ip }}
        username: ubuntu
        key: ${{ secrets.EC2_SSH_KEY }}
        timeout: 30m
        command_timeout: 30m
        envs: AWS_ACCESS_KEY_ID,AWS_SECRET_ACCESS_KEY,AWS_DEFAULT_REGION,UNIQUE_SUFFIX
        script: |
          echo "=== Starting EC2 Service Deployment ==="
          
          # Wait for setup completion with longer timeout
          echo "Waiting for EC2 initialization to complete..."
          timeout=1200  # 20 minutes
          while [ ! -f /home/ubuntu/setup-info.txt ] && [ $timeout -gt 0 ]; do
            echo "Waiting for setup completion... ($timeout seconds remaining)"
            sleep 20
            timeout=$((timeout-20))
          done
          
          if [ ! -f /home/ubuntu/setup-info.txt ]; then
            echo "âš ï¸  Setup timeout - proceeding with manual setup..."
            
            # Manual Docker installation (no apt-get)
            if ! command -v docker &> /dev/null; then
              echo "Installing Docker manually..."
              curl -fsSL https://get.docker.com -o get-docker.sh
              sudo sh get-docker.sh
              sudo usermod -aG docker ubuntu
              sudo systemctl start docker
              sudo systemctl enable docker
            fi
            
            # Manual Docker Compose installation
            if ! command -v docker-compose &> /dev/null; then
              echo "Installing Docker Compose..."
              sudo curl -L "https://github.com/docker/compose/releases/download/v2.24.0/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
              sudo chmod +x /usr/local/bin/docker-compose
            fi
            
            # Manual AWS CLI installation (no apt-get)
            if ! command -v aws &> /dev/null; then
              echo "Installing AWS CLI..."
              curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
              unzip awscliv2.zip
              sudo ./aws/install
            fi
          else
            echo "âœ… EC2 setup completed successfully!"
            cat /home/ubuntu/setup-info.txt
          fi
          
          # Ensure Docker is running
          echo "Starting Docker service..."
          sudo systemctl start docker
          sudo systemctl enable docker
          
          # Wait for Docker to be ready
          echo "Waiting for Docker to be ready..."
          timeout 300 bash -c 'while ! sudo docker info > /dev/null 2>&1; do echo "Waiting for Docker..."; sleep 5; done'
          
          if sudo docker info > /dev/null 2>&1; then
            echo "âœ… Docker is ready!"
          else
            echo "âŒ Docker failed to start"
            sudo systemctl status docker
            exit 1
          fi
          
          # Configure AWS CLI with credentials from environment
          echo "Configuring AWS CLI with provided credentials..."
          aws configure set aws_access_key_id "$AWS_ACCESS_KEY_ID"
          aws configure set aws_secret_access_key "$AWS_SECRET_ACCESS_KEY"
          aws configure set default.region ap-southeast-1
          
          # Test AWS credentials
          echo "Testing AWS credentials..."
          if aws sts get-caller-identity; then
            echo "âœ… AWS credentials working"
          else
            echo "âŒ AWS credentials not working"
            exit 1
          fi
          
          # Login to ECR
          echo "Logging into ECR..."
          aws ecr get-login-password --region ap-southeast-1 | sudo docker login --username AWS --password-stdin $(aws sts get-caller-identity --query Account --output text).dkr.ecr.ap-southeast-1.amazonaws.com
          
          # Create monitoring directories
          echo "Setting up monitoring directories..."
          mkdir -p /home/ubuntu/monitoring/prometheus
          mkdir -p /home/ubuntu/monitoring/grafana/provisioning/datasources
          mkdir -p /home/ubuntu/monitoring/grafana/provisioning/dashboards
          mkdir -p /home/ubuntu/monitoring/grafana/dashboards
          
          # Create Prometheus config
          cat > /home/ubuntu/monitoring/prometheus/prometheus.yml << 'PROMETHEUS_EOF'
          global:
            scrape_interval: 15s
            evaluation_interval: 15s
          
          scrape_configs:
            - job_name: 'prometheus'
              static_configs:
                - targets: ['localhost:9090']
          
            - job_name: 'ml-inference'
              static_configs:
                - targets: ['ml-inference:8001']
              metrics_path: '/metrics'
              scrape_interval: 10s
          
            - job_name: 'data-ingestion'
              static_configs:
                - targets: ['data-ingestion:8002']
              metrics_path: '/metrics'
              scrape_interval: 10s
          PROMETHEUS_EOF
          
          # Create Grafana datasource config
          cat > /home/ubuntu/monitoring/grafana/provisioning/datasources/prometheus.yml << 'DATASOURCE_EOF'
          apiVersion: 1
          datasources:
            - name: Prometheus
              type: prometheus
              access: proxy
              url: http://prometheus:9090
              isDefault: true
              editable: false
          DATASOURCE_EOF
          
          # Create Grafana dashboard provisioning config
          cat > /home/ubuntu/monitoring/grafana/provisioning/dashboards/dashboard.yml << 'DASHBOARD_EOF'
          apiVersion: 1
          providers:
            - name: 'MLOps Dashboards'
              orgId: 1
              folder: ''
              type: file
              disableDeletion: false
              updateIntervalSeconds: 10
              allowUiUpdates: true
              options:
                path: /var/lib/grafana/dashboards
                foldersFromFilesStructure: true
          DASHBOARD_EOF
          
          # Get ECR repository URLs
          echo "Using unique suffix: $UNIQUE_SUFFIX"
          
          ML_INFERENCE_REPO=$(aws ecr describe-repositories --repository-names "mlops/ml-inference-$UNIQUE_SUFFIX" --query 'repositories[0].repositoryUri' --output text 2>/dev/null || echo "")
          DATA_INGESTION_REPO=$(aws ecr describe-repositories --repository-names "mlops/data-ingestion-$UNIQUE_SUFFIX" --query 'repositories[0].repositoryUri' --output text 2>/dev/null || echo "")
          
          if [ -z "$ML_INFERENCE_REPO" ] || [ -z "$DATA_INGESTION_REPO" ]; then
            echo "âŒ Error: Could not retrieve ECR repository URLs"
            echo "Available repositories:"
            aws ecr describe-repositories --query 'repositories[].repositoryName' --output text
            exit 1
          fi
          
          echo "âœ… Repository URLs retrieved:"
          echo "  ML Inference: $ML_INFERENCE_REPO"
          echo "  Data Ingestion: $DATA_INGESTION_REPO"
          
          # Create docker-compose.yml
          cat > /home/ubuntu/docker-compose.yml << COMPOSE_EOF
          version: '3.8'
          services:
            ml-inference:
              image: ${ML_INFERENCE_REPO}:latest
              ports:
                - "8001:8001"
              restart: always
              networks:
                - mlops-network
              healthcheck:
                test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
                interval: 30s
                timeout: 10s
                retries: 3
                start_period: 90s
                
            data-ingestion:
              image: ${DATA_INGESTION_REPO}:latest
              ports:
                - "8002:8002"
              restart: always
              networks:
                - mlops-network
              healthcheck:
                test: ["CMD", "curl", "-f", "http://localhost:8002/health"]
                interval: 30s
                timeout: 10s
                retries: 3
                start_period: 90s
                
            prometheus:
              image: prom/prometheus:latest
              ports:
                - "9090:9090"
              volumes:
                - ./monitoring/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
                - prometheus-data:/prometheus
              command:
                - '--config.file=/etc/prometheus/prometheus.yml'
                - '--storage.tsdb.path=/prometheus'
                - '--web.console.libraries=/etc/prometheus/console_libraries'
                - '--web.console.templates=/etc/prometheus/consoles'
                - '--storage.tsdb.retention.time=200h'
                - '--web.enable-lifecycle'
              restart: always
              networks:
                - mlops-network
                
            grafana:
              image: grafana/grafana:latest
              ports:
                - "3000:3000"
              environment:
                - GF_SECURITY_ADMIN_PASSWORD=admin
                - GF_USERS_ALLOW_SIGN_UP=false
              volumes:
                - ./monitoring/grafana/provisioning:/etc/grafana/provisioning
                - ./monitoring/grafana/dashboards:/var/lib/grafana/dashboards
                - grafana-data:/var/lib/grafana
              depends_on:
                - prometheus
              restart: always
              networks:
                - mlops-network
                
          networks:
            mlops-network:
              driver: bridge
              
          volumes:
            prometheus-data:
            grafana-data:
          COMPOSE_EOF
          
          # Pull latest images
          echo "ğŸ“¥ Pulling latest Docker images..."
          sudo docker pull ${ML_INFERENCE_REPO}:latest || exit 1
          sudo docker pull ${DATA_INGESTION_REPO}:latest || exit 1
          
          # Deploy services
          echo "ğŸš€ Deploying MLOps services..."
          cd /home/ubuntu
          sudo docker-compose down || true
          sudo docker-compose up -d
          
          # Wait for services to start
          echo "â³ Waiting for services to initialize..."
          sleep 120
          
          # Check service health
          echo "ğŸ” Checking service health..."
          success=false
          for i in {1..20}; do
            echo "Health check attempt $i/20..."
            
            ML_HEALTH=$(curl -s -o /dev/null -w "%{http_code}" http://localhost:8001/health 2>/dev/null || echo "000")
            DATA_HEALTH=$(curl -s -o /dev/null -w "%{http_code}" http://localhost:8002/health 2>/dev/null || echo "000")
            
            echo "  ğŸ¤– ML Inference: $ML_HEALTH"
            echo "  ğŸ“Š Data Ingestion: $DATA_HEALTH"
            
            if [ "$ML_HEALTH" = "200" ] && [ "$DATA_HEALTH" = "200" ]; then
              echo "âœ… All core services are healthy!"
              success=true
              break
            else
              echo "â³ Services starting up, waiting 25 seconds..."
              sleep 25
            fi
          done
          
          if [ "$success" = true ]; then
            echo ""
            echo "ğŸ‰ MLOps Pipeline Deployment Successful!"
            echo "=================================================="
            
            PUBLIC_IP=$(curl -s ifconfig.me 2>/dev/null || echo "UNKNOWN")
            echo "ğŸŒ Public IP: $PUBLIC_IP"
            echo ""
            echo "ğŸ“Š Service Access URLs:"
            echo "  ğŸ¤– ML Inference Service:  http://$PUBLIC_IP:8001"
            echo "  ğŸ“ˆ Data Ingestion Service: http://$PUBLIC_IP:8002"
            echo "  ğŸ“Š Prometheus Monitoring:  http://$PUBLIC_IP:9090"
            echo "  ğŸ“ˆ Grafana Dashboard:      http://$PUBLIC_IP:3000 (admin/admin)"
            echo ""
            echo "ğŸ”— Test your services:"
            echo "  curl http://$PUBLIC_IP:8001/health"
            echo "  curl http://$PUBLIC_IP:8002/health"
            echo ""
            echo "âœ… Deployment completed successfully!"
          else
            echo "âŒ Service health check failed after 20 attempts"
            echo ""
            echo "ğŸ” Container status:"
            sudo docker-compose ps
            echo ""
            echo "ğŸ“‹ Recent container logs:"
            sudo docker-compose logs --tail=15
            exit 1
          fi
